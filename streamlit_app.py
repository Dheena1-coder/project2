import streamlit as st
import faiss
from sentence_transformers import SentenceTransformer
from nltk.tokenize import sent_tokenize
import tempfile
import fitz  # PyMuPDF
from io import BytesIO
from PIL import Image
import pandas as pd

# Load the transformer model for embeddings
model = SentenceTransformer('all-MiniLM-L6-v2')

# Load the SFDR and Asset Keyword data from GitHub (URLs directly)
def load_keywords_from_github(url):
    # Load the Excel file directly from GitHub
    df = pd.read_excel(url, engine='openpyxl')  
    return df

# Process data into dictionary
def process_keywords_to_dict(df, team_type):
    keyword_dict = {}
    for index, row in df.iterrows():
        indicator = row['SFDR Indicator'] if team_type == 'sfdr' else row['Asset Type']
        datapoint_name = row['Datapoint Name']
        keywords = row['Keywords'].split(',')
        keywords = [keyword.strip() for keyword in keywords]

        if indicator not in keyword_dict:
            keyword_dict[indicator] = {}

        if datapoint_name not in keyword_dict[indicator]:
            keyword_dict[indicator][datapoint_name] = []

        keyword_dict[indicator][datapoint_name].extend(keywords)

    # Optional: Remove duplicates within each list of keywords for each Datapoint Name
    for indicator in keyword_dict:
        for datapoint in keyword_dict[indicator]:
            keyword_dict[indicator][datapoint] = list(set(keyword_dict[indicator][datapoint]))

    return keyword_dict

# Function to upload PDF
def upload_pdf():
    uploaded_file = st.file_uploader("Upload PDF", type="pdf")
    if uploaded_file is not None:
        return uploaded_file
    return None

# Function to extract text and word positions from PDF (with page number tracking)
def extract_pdf_content(pdf_file):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
        temp_file.write(pdf_file.read())
        temp_file_path = temp_file.name
        
    doc = fitz.open(temp_file_path)
    text_chunks = []
    word_positions = []  # To store word positions for highlighting
    
    # Extract text and word positions from each page
    for page_number, page in enumerate(doc, start=1):
        text = page.get_text("text")  # Extract text from the page
        words = page.get_text("words")  # Extract word positions
        
        # Store each sentence with its respective page and word positions
        text_chunks.extend([(sent, page_number, words) for sent in sent_tokenize(text)])
    
    return text_chunks, doc  # Return text with word positions and the document object for highlighting

# Function to create embeddings
def create_embeddings(text_chunks):
    text_only = [chunk[0] for chunk in text_chunks]  # Extract sentences (text only) from tuples
    embeddings = model.encode(text_only)
    return embeddings

# Function to build vector database
def build_vector_database(embeddings):
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    return index

# Function to retrieve context based on user query
def retrieve_context(query, text_chunks, index):
    query_embedding = model.encode([query])
    distances, indices = index.search(query_embedding, k=3)
    return [(text_chunks[i][0], text_chunks[i][1], text_chunks[i][2], distances[0][j]) for j, i in enumerate(indices[0])]

# Function to highlight matching words in the PDF
def highlight_text_on_pdf(doc, query, page_number):
    page = doc.load_page(page_number - 1)  # Page numbers are 0-indexed in PyMuPDF
    
    # Search for the query on the page
    text_instances = page.search_for(query)  # This finds the exact position of the query in the page's text
    
    # Loop through all the instances and draw highlights around them
    for inst in text_instances:
        rect = fitz.Rect(inst)  # Create a rectangle based on the text instance
        page.draw_rect(rect, color=(0, 1, 0), width=2)  # Draw a green rectangle around the keyword (no fill)
    
    return doc

# Function to convert page to image with highlights
def page_to_image_with_highlights(doc, page_number):
    page = doc.load_page(page_number - 1)  # Page numbers are 0-indexed in PyMuPDF
    img = page.get_pixmap()  # Get the page as a pixmap (image)
    return Image.open(BytesIO(img.tobytes()))  # Convert to image

# Function to calculate keyword statistics (frequency of occurrence)
def calculate_keyword_statistics(text_chunks, selected_keywords):
    keyword_stats = {}
    
    # Initialize stats dictionary for each selected keyword
    for keyword in selected_keywords:
        keyword_stats[keyword] = {
            'occurrences': 0,
            'pages': set()
        }
    
    # Count occurrences of keywords in the text chunks
    for sentence, page_number, _ in text_chunks:
        for keyword in selected_keywords:
            if keyword.lower() in sentence.lower():
                keyword_stats[keyword]['occurrences'] += 1
                keyword_stats[keyword]['pages'].add(page_number)
    
    return keyword_stats

def main():
    # Streamlit UI components
    st.title("ðŸ“„ **PDF Keyword Extractor **")
    st.markdown("This tool helps you extract text and their respective page from PDFs and search for specific keywords. The matched keywords will be highlighted in the pdf page and text along with their surrounding context.")
    
    # Upload the PDF file
    pdf_file = upload_pdf()

    # Check if PDF is uploaded
    if pdf_file is None:
        st.warning("Please upload a PDF file.")
        return

    # URLs of the GitHub Excel files (update with actual raw GitHub links)
    sfdr_file_url = "https://raw.github.com/Dheena1-coder/PdfAnalyzer/master/sfdr_file.xlsx"  # Replace with actual SFDR Excel file URL
    asset_file_url = "https://raw.github.com/Dheena1-coder/PdfAnalyzer/master/asset_file.xlsx"  # Replace with actual Asset Excel file URL

    # Load and process the keyword dictionaries
    sfdr_df = load_keywords_from_github(sfdr_file_url)
    asset_df = load_keywords_from_github(asset_file_url)

    sfdr_keywords_dict = process_keywords_to_dict(sfdr_df, 'sfdr')
    asset_keywords_dict = process_keywords_to_dict(asset_df, 'assets')

    # Create dropdown for team selection
    team_type = st.selectbox("Select Team", ["sfdr", "physical assets"])

    # Display appropriate keyword dictionary based on team selection
    if team_type == "sfdr":
        indicators = list(sfdr_keywords_dict.keys())
    else:
        indicators = list(asset_keywords_dict.keys())
    
    indicator = st.selectbox("Select Indicator", indicators)
    if team_type == "sfdr":
        datapoint_names = list(sfdr_keywords_dict[indicator].keys())
    else:
        datapoint_names = list(asset_keywords_dict[indicator].keys())
    
    datapoint_name = st.multiselect("Select Datapoint Names", datapoint_names)
    
    # Keyword Text Area: Allow users to add additional keywords
    extra_keywords_input = st.text_area("Additional Keywords (comma-separated)", "")

    # Extract relevant keywords based on the selected datapoint names
    selected_keywords = []
    if team_type == "sfdr":
        for datapoint in datapoint_name:
            selected_keywords.extend(sfdr_keywords_dict[indicator].get(datapoint, []))
    else:
        for datapoint in datapoint_name:
            selected_keywords.extend(asset_keywords_dict[indicator].get(datapoint, []))

    selected_keywords = list(set(selected_keywords))  # Remove duplicates
    
    # Add any extra keywords entered in the text area
    if extra_keywords_input:
        extra_keywords = [keyword.strip() for keyword in extra_keywords_input.split(',')]
        selected_keywords.extend(extra_keywords)

    selected_keywords = list(set(selected_keywords))  # Remove duplicates after adding extra keywords
    st.write(selected_keywords)

    # Ensure the PDF file is loaded and text chunks are extracted after the upload
    if pdf_file:
        text_chunks, doc = extract_pdf_content(pdf_file)  # Extract text and word positions

        # Check if text_chunks were successfully extracted
        if not text_chunks:
            st.warning("No text extracted from the PDF. Please upload a valid PDF.")
            return

        embeddings = create_embeddings(text_chunks)
        index = build_vector_database(embeddings)  

        # Calculate keyword statistics
        keyword_stats = calculate_keyword_statistics(text_chunks, selected_keywords)
        
        # Display keyword statistics
        st.write("### Keyword Statistics")
        stats_data = []
        for keyword, stats in keyword_stats.items():
            stats_data.append([keyword, stats['occurrences'], sorted(list(stats['pages']))])
        
        stats_df = pd.DataFrame(stats_data, columns=["Keyword", "Occurrences", "Pages"])
        st.dataframe(stats_df)

        # User input query
        query = st.text_input("Enter your query:")
        if query:
            results = retrieve_context(query, text_chunks, index)
            for result in results:
                # Display relevant text with page number
                st.write(f"**Page {result[1]}**: {result[0]}")  # Display relevant sentence
                page_number = result[1]
                
                # Highlight matching words and generate image of the page
                doc_with_highlights = highlight_text_on_pdf(doc, query, page_number)
                highlighted_image = page_to_image_with_highlights(doc_with_highlights, page_number)
                
                # Display the page with highlights
                st.image(highlighted_image, caption=f"Highlighted Page {page_number}")



if __name__ == "__main__":
    main()
